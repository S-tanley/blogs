<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Reading Notes for Orca | Stanley's Blog</title><meta name="author" content="Stanley Zheng"><meta name="copyright" content="Stanley Zheng"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="This is the reading notes for the ORCA: A Distributed Serving System for Transformer-Based Generative Models. This is an OSDI conference paper from 2022. Almost all the authors come from South Korea,">
<meta property="og:type" content="article">
<meta property="og:title" content="Reading Notes for Orca">
<meta property="og:url" content="https://s-tanley.github.io/blogs/2025/05/15/Orca/index.html">
<meta property="og:site_name" content="Stanley&#39;s Blog">
<meta property="og:description" content="This is the reading notes for the ORCA: A Distributed Serving System for Transformer-Based Generative Models. This is an OSDI conference paper from 2022. Almost all the authors come from South Korea,">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://s-tanley.github.io/blogs/img/MyProfilePicture.JPG">
<meta property="article:published_time" content="2025-05-15T05:00:00.000Z">
<meta property="article:modified_time" content="2025-06-08T14:18:09.255Z">
<meta property="article:author" content="Stanley Zheng">
<meta property="article:tag" content="MLSys">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s-tanley.github.io/blogs/img/MyProfilePicture.JPG"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Reading Notes for Orca",
  "url": "https://s-tanley.github.io/blogs/2025/05/15/Orca/",
  "image": "https://s-tanley.github.io/blogs/img/MyProfilePicture.JPG",
  "datePublished": "2025-05-15T05:00:00.000Z",
  "dateModified": "2025-06-08T14:18:09.255Z",
  "author": [
    {
      "@type": "Person",
      "name": "Stanley Zheng",
      "url": "https://s-tanley.github.io/blogs/"
    }
  ]
}</script><link rel="shortcut icon" href="/blogs/img/favicon.png"><link rel="canonical" href="https://s-tanley.github.io/blogs/2025/05/15/Orca/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/blogs/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/blogs/',
  algolia: undefined,
  localSearch: {"path":"/blogs/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"No results found for: ${query}","hits_stats":"${hits} articles found"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Reading Notes for Orca',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/blogs/img/MyProfilePicture.JPG" onerror="this.onerror=null;this.src='/blogs/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/blogs/archives/"><div class="headline">Articles</div><div class="length-num">55</div></a><a href="/blogs/tags/"><div class="headline">Tags</div><div class="length-num">15</div></a><a href="/blogs/categories/"><div class="headline">Categories</div><div class="length-num">8</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="https://s-tanley.github.io"><i class="fa-fw fas fa-id-badge"></i><span> Stanley Zheng</span></a></div><div class="menus_item"><a class="site-page" href="/blogs/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/blogs/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/blogs/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/blogs/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/blogs/"><img class="site-icon" src="/blogs/img/MyProfilePicture.JPG" alt="Logo"><span class="site-name">Stanley's Blog</span></a><a class="nav-page-title" href="/blogs/"><span class="site-name">Reading Notes for Orca</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="https://s-tanley.github.io"><i class="fa-fw fas fa-id-badge"></i><span> Stanley Zheng</span></a></div><div class="menus_item"><a class="site-page" href="/blogs/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/blogs/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/blogs/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/blogs/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Reading Notes for Orca</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-05-15T05:00:00.000Z" title="Created 2025-05-15 00:00:00">2025-05-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-06-08T14:18:09.255Z" title="Updated 2025-06-08 09:18:09">2025-06-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/blogs/categories/Reading-Paper/">Reading Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>This is the reading notes for the <em>ORCA: A Distributed Serving System for Transformer-Based Generative Models</em>. This is an OSDI conference paper from 2022. Almost all the authors come from South Korea, and actually, this is the first time I have read papers written by Koreans.</p>
<h1>Summary</h1>
<h2 id="Abstract-Introduction-Background">Abstract &amp; Introduction &amp; Background</h2>
<p>The paper is focused on the inference serving, they point out that the existing system is not good enough for transformer-based models. So, they propose a new method called <strong>iteration-level scheduling</strong> and create a distributed system called <strong>ORCA</strong>.</p>
<p>Besides the scheduling thing, they also introduced a new batching method to fit with their scheduling method, which is called <strong>selective batching</strong>.</p>
<p>The background information they give is kind of detailed. The first part is about the transformer-based models, the second part is about the existing serving system.</p>
<img src="./Orca/CleanShot 2025-05-25 at 14.20.44@2x.png" alt="CleanShot 2025-05-25 at 14.20.44@2x" style="zoom:80%;" />
<p>The Figure 1(a) actually brings me some confusion, and I talked about it in Thoughts &gt; About Figure 1.</p>
<p>The background information do not have too much to talk about.</p>
<p>There are also two figures about exising inference system.</p>
<img src="./Orca/CleanShot 2025-05-25 at 14.35.46@2x.png" alt="CleanShot 2025-05-25 at 14.35.46@2x" style="zoom:35%;" />
<img src="./Orca/CleanShot 2025-05-25 at 14.36.19@2x.png" alt="CleanShot 2025-05-25 at 14.36.19@2x" style="zoom:35%;" />
<p>Figure 2 is how the existing serving system work, and the Figure 3 is why the current serving system oot good enough. The figure is actually kind of clear.</p>
<p>Basically, we just combine requests in to a batch, the send the batch to the model, and after all the requests meet the end, we send them back. Some request may need more iteration, but the other requests can only wait. This is what the authors want to improve.</p>
<h2 id="Challenges-and-Proposed-Solutions">Challenges and Proposed Solutions</h2>
<p>Just two challenges and two corresponding solutions.</p>
<p>First about schedling and second about batching.</p>
<img src="./Orca/CleanShot 2025-05-25 at 14.44.21@2x.png" alt="CleanShot 2025-05-25 at 14.44.21@2x" style="zoom:35%;" />
<p>They want for each iteration, we can choose the requests, so they create a request pool. Every iteration, they choose enough requests from the pool to make up a batch, and after only one iteration, the model sends the output back to the pool. The requests that are done can be sent back to the clients, and new requests can just be put in the pool.</p>
<img src="./Orca/CleanShot 2025-05-26 at 11.28.58@2x.png" alt="CleanShot 2025-05-26 at 11.28.58@2x" style="zoom:33%;" />
<p>The second thing they do is the selective batching, just like Figure 5 shows above. The basic idea is simple, but there are some details I am confused about. I write them in Thoughts &gt; Confusion &gt; About batching.</p>
<h2 id="ORCA-Design-Implementation">ORCA Design &amp; Implementation</h2>
<p>The ORCA design basically contains two things: one is how we place our model on the GPUs, and the other one is what is the detail of the scheduling.</p>
<img src="./Orca/CleanShot 2025-05-26 at 22.31.56@2x.png" alt="CleanShot 2025-05-26 at 22.31.56@2x" style="zoom:33%;" />
<p>The Figure 6 and 7 introduce how ORCA places the model on GPUs and what parallelization strategy ORCA uses. About parallelism, there are so many different terms about this, but they all represent the same thing.</p>
<img src="./Orca/CleanShot 2025-05-27 at 11.19.42@2x.png" alt="CleanShot 2025-05-27 at 11.19.42@2x" style="zoom:33%;" />
<p>Here, for ORCA, every intra-layer will be set as one node, just like Figure 7 shows. It’s very clear, not like some other papers, very blurry about what a “worker” or “node” actually refers to.</p>
<p>For the scheduling algorithm, it’s just very detailed, and there’s no point I talking here. I have some questions about this, and write them in Thoughts &gt; Confusion &gt; About the scheduling algorithms.</p>
<p>The implementation is very short and do not have too much to talk about.</p>
<h2 id="Evaluation-Related-Work-and-Discussion-Conclusion">Evaluation &amp; Related Work and Discussion &amp; Conclusion</h2>
<p>The evaluation is just you set up a baseline and compare it with different setups, and show that your results are great.</p>
<h1>Thoughts</h1>
<p>The paper is kind of old, I think nowadays the support for the LLM inference is very sophisticated. I know two mainstream open-source projects focus on this area: vLLm and SGLang. I actually know some developers of SGLang. However, in this paper, none of them is mentioned 😂. I think this is because the paper is kind of old, and for the framework it mentions, I actually have no idea.</p>
<h2 id="Confusion">Confusion</h2>
<h3 id="About-Figure-1">About Figure 1</h3>
<p>Figure 1(a) was initially a little confusing for me, as it seemed that in the increment phase, we only needed the output from the last iteration. However, what I remember about how the transformer works is that you add the last iteration’s output to your last iteration’s input. Like in the initial phase, your input is “I think it”, the output is “is”, then the next iteration’s input will be “I think it is” and so on. If you read some other papers or some other blogs about the transformer, you’ll see some figures drawn in this way.</p>
<p>I think this is about two different ways to express the model. Each iteration definitely uses the information of the previous iteration’s input, but in this paper, the author may think this information is already in our K/V cache or K/V manager. It’s like not purely input, so they just ignore these. However, I think the most plausible answer to me is that omitting the previous input makes it easy to draw good figures😂.</p>
<p>This is actually related to another topic, which is “why in an inference system, we only need to store K/V?” There is a <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/23553666052">blog</a> that talks about this issue.</p>
<h3 id="About-batching">About batching</h3>
<h4 id="How-to-batch-requests-together">How to batch requests together</h4>
<p>In this paper, it says that if we satisfy some strict conditions, we can batch different requests together, and I just a little bit confused about that, since for me it seems that there is no way we can do the attention process together.</p>
<h4 id="About-Figure-5">About Figure 5</h4>
<p>I just don’t know why the hidden dimension becomes 3H.</p>
<h3 id="About-the-scheduling-algorithms">About the scheduling algorithms</h3>
<h4 id="first-come-first-served-FCFS">first-come-first-served (FCFS)</h4>
<p>They mention that they make sure that the previous come requests must have greater or equal iteration times than the later requests. However, the detail is omitted, I just wonder how they do this, like sort the request pool every iteration? Just wondering.</p>
<h4 id="max-tokens-attribute"><em>max_tokens</em> attribute</h4>
<p>This is a very important attribute, but they do not state how they get this attribute. It seems that they just know the maximum output length of each requests from nowhere.</p>
<h2 id="Ideas">Ideas</h2>
<h3 id="About-the-iteration-level-scheduling">About the &quot;iteration level scheduling &quot;</h3>
<p>They basically run one iteration of work for a request, then check its status, managing all these requests in a central pool. This “iterate once, check once” model is efficient in some ways, but it sparked a thought: could we make it even smarter?</p>
<p>What if we could get a rough idea, right at the start, of how many iterations a particular request might need to finish? Maybe a small, lightweight predictive model – something like a mini neural network – could look at a request’s characteristics and make an educated guess. It could flag some as likely “quick wins” and others as “longer hauls.”</p>
<p>If we had that kind of foresight, we could then route requests into different processing pools based on their predicted effort. Imagine a pool for tasks expected to finish in just a few iterations, another for medium-length ones, and a third for those predicted to take a significant amount of time. The real tweak would then be how often we “check in” on the progress of requests in each pool. For the fast pool, frequent checks, similar to Orca’s current method, would make sense to maintain responsiveness. But for requests in the medium or long-haul pools, we could reduce the check-in frequency considerably. Instead of checking after every single iteration, perhaps we’d check after every K1 iterations for medium tasks, and an even larger K2 iterations for the long ones.</p>
<p>The potential upside here is a reduction in overhead. Constantly checking the status of a long-running task can be wasteful. By being more selective about when we check, especially for tasks we anticipate will take a while, the system could spend more of its resources actually <em>doing</em> the work rather than managing it. This could lead to better throughput for longer jobs without significantly impacting the latency of shorter ones. Naturally, the accuracy of the predictive model and the overhead of managing multiple pools with different check strategies would be key things to figure out, but it’s an intriguing possibility for optimizing such iterative processing systems. It’s just a thought bubble for now, but an interesting one!</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://s-tanley.github.io/blogs">Stanley Zheng</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://s-tanley.github.io/blogs/2025/05/15/Orca/">https://s-tanley.github.io/blogs/2025/05/15/Orca/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/blogs/tags/MLSys/">MLSys</a></div><div class="post-share"><div class="social-share" data-image="/blogs/img/MyProfilePicture.JPG" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/blogs/2025/05/07/%E7%94%9F%E6%B4%BB%E6%97%A5%E5%BF%97(2025-5-7)/" title="生活日志(2025-5-7)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">生活日志(2025-5-7)</div></div><div class="info-2"><div class="info-item-1">Final Week 了，最近可以说是非常忙碌了。回国的机票都买完了，要坐将近两天飞机，感觉也是蛮离谱的。CS 525 也考完了，感觉应该还行，应该可以拿 A。CS 475 和 STAT 333 就剩一个尾巴了，所以只剩一个复习 354 了，希望问题不大，还没开始复习，笑死了，感觉东西还是挺多的。 不过今天心情还蛮不错的，有点好消息。除了这些考试都要考完了，我非常久之前申请的暑期的线上 TA 实习也终于有消息了，还以为直接无了，现在至少是第一候选人了。MBZUAI 也终于回复我了一个邮件，说明一切应该都还在 in process，但是快 20 天才回也是怪离谱的。CS 354 的 PM 也面试完了，整整面试了一个小时，也是比较抽象了，我记得这个时间也是最后一个可选的时间段，可能会很快出结果吧，跟 instructor 还是挺熟的，希望能有好的结果🙏。 加油了，也不知道一天够不够复习的，感觉够呛啊哈哈哈哈哈哈，又要焦虑了，毕竟一个 quiz 就能写很久。 最近还一直在看组 nas 或者组 GPU，回家之后打算先把我的 Mac mini 小主机用上，组一个 nas...</div></div></div></a><a class="pagination-related" href="/blogs/2025/05/16/caffeinate%20%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/" title="Mac 好用的命令行工具总结"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">Mac 好用的命令行工具总结</div></div><div class="info-2"><div class="info-item-1">把一些可能用到的比较好用的工具总结到了这里。 使用 rsync 进行高效文件同步与恢复（macOS） rsync 是 macOS 和 Linux 系统中常用的命令行文件同步工具，支持断点续传、增量复制、排除文件等功能，非常适合拷贝大文件或进行数据迁移。  一、基本语法 rsync [选项] 源路径 目标路径 源路径和目标路径都可以是本地路径或远程路径（使用 SSH）。拷贝路径时注意 / 的使用影响结构，详见后文。 二、常用选项说明    选项 含义     -a 归档模式（保留权限、时间戳、符号链接等）   -v 输出详细信息（verbose）   -h 以人类可读的方式显示大小（如 1K、20M）   –progress 显示每个文件的复制进度   –dry-run 预演命令但不执行操作，适合拷贝前查看将会做什么   –delete 删除目标路径中，源路径中已不存在的文件（谨慎使用）   –update 只复制源路径中比目标路径更新的文件   –exclude 排除某些文件或目录，例如 .DS_Store    三、路径末尾 / 的含义  不加...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/blogs/2025/04/07/AllReduce%20&%20Bucketing/" title="AllReduce &amp; Bucketing"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-07</div><div class="info-item-2">AllReduce &amp; Bucketing</div></div><div class="info-2"><div class="info-item-1">本文主要介绍了 AllReduce 和 Bucketing 分别是什么，和他们之间的联系。 一、AllReduce 是什么？ AllReduce 是分布式训练中的一种集体通信操作， 用于在多个 GPU（worker）之间同步张量（通常是梯度）。 典型流程如下：  每个 GPU 独立计算自己的梯度张量（如 grad）。 所有 GPU 通过 AllReduce 操作，将各自的张量求和/平均，获得全局一致的梯度。 每个 GPU 使用这个同步后的梯度更新模型参数。  AllReduce 是数据并行训练中实现模型同步的关键机制。  二、为什么 AllReduce 会成为性能瓶颈？  模型中参数众多，梯度张量数量也很多。 每个张量如果单独 AllReduce，通信次数极多。 小张量通信无法充分利用带宽，且频繁启动通信带来显著延迟（latency）。   三、Bucketing 是什么？ Bucketing 是一种优化 AllReduce 通信效率的策略， 将多个小张量合并成一个大 “bucket”，再一次性执行 AllReduce。 核心思想：Batch Small Reduces...</div></div></div></a><a class="pagination-related" href="/blogs/2025/03/28/FasterMoE/" title="Reading Notes for FasterMoE"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-28</div><div class="info-item-2">Reading Notes for FasterMoE</div></div><div class="info-2"><div class="info-item-1">Summary Abstract &amp; Introduction &amp;  Background and Challenges 前面又是简单介绍MoE，基本都一样。 这个也是training方向的，说了三个challenges：   dynamic load imbalance 在intro里，叫Dynamic expert selection，就也比较明显，就是每次选的experts不一样。   inefficient synchronous execution mode 在intro里，叫Inefficient synchronous operations，就是expert有dependency，就需要别的worker的data，要等。   congested all-to-all communication 在intro里，叫Mismatch of model design and network topology，感觉他的意思是现在的system只管摆放experts的computation...</div></div></div></a><a class="pagination-related" href="/blogs/2025/04/07/MoE%20%E4%B8%AD%20All-to-All%20%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6/" title="MoE 中 All-to-All 通信机制"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-07</div><div class="info-item-2">MoE 中 All-to-All 通信机制</div></div><div class="info-2"><div class="info-item-1">本文主要介绍了 All-to-All 通信机制，以及为什么需要这个机制。 一、All-to-All 是什么？ 在分布式 Mixture-of-Experts（MoE）模型中，All-to-All 是一种通信操作， 用于在多个 GPU 之间交换 token 和专家（Expert）之间的数据。 每个 GPU 上都有输入 token，而每个 Expert 分布在多个不同的 GPU 上。 Gate 网络决定每个 token 应该由哪些专家处理，因此 token 需要被动态发送到目标 Expert 所在的 GPU。 这正是 All-to-All：每个 GPU 既向其他 GPU 发送数据，也接收来自其他 GPU 的数据。  二、为什么 MoE 模型需要 All-to-All？ 1. Expert 是独立的，但 Token 是全局的  每个 Expert 的参数是本地的，只存在于某个 GPU 上。 但 token 是通过数据并行划分的，分布在所有 GPU 上。 每个 token 的 gate 结果可能指向任意 GPU 上的 Expert。  因此，token 必须被跨设备发送到它所选中的...</div></div></div></a><a class="pagination-related no-desc" href="/blogs/2025/05/27/SGLang/" title="Reading Notes for SGLang"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-27</div><div class="info-item-2">Reading Notes for SGLang</div></div></div></a><a class="pagination-related" href="/blogs/2025/06/10/Resource%20I%20have%20for%20MLSys/" title="Resource I Have for MLSys"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-10</div><div class="info-item-2">Resource I Have for MLSys</div></div><div class="info-2"><div class="info-item-1">This is like a guidance page for the resources I know for MLSys, I’ll give a brief introduction to each of them and list the link here. The resources will contain books, papers, and notes I wrote. Books AI System This book is more about the hardware. I think it’s a little bit like for ECE students. I haven’t read it all yet, but I think you can find some useful topics here, such as the introduction to Nvidia GPUs, the Tensor Core, stream multiprocessors, and how the GPU actually do to...</div></div></div></a><a class="pagination-related" href="/blogs/2025/03/27/SmartMoE/" title="Reading Notes for SmartMoE"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-27</div><div class="info-item-2">Reading Notes for SmartMoE</div></div><div class="info-2"><div class="info-item-1">Summary Abstract &amp; Introduction &amp; Background and Motivation Deep neural network（DNN）现在越来越大，除了dense model，就是比较传统的model之外，越来越多的人开始关注sparsely activated model。针对dense model，之前有很多auto-parallelization的方法，但是这些方法对sparsely activated model，比如说MoE架构的模型就没那么好用了。所以他们主要做的就是实现对sparsely activated model做自动并行的分布式训练的方法。 Intro就先说一下来龙去脉，就众所周知，scaling law目前对DNN一直没有失效，所以各家基本上就是一直往上堆参数。但模型变大了就练不动了，所以就要找efficient...</div></div></div></a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comments</span></div><div class="comment-switch"><span class="first-comment">Giscus</span><span id="switch-btn"></span><span class="second-comment">Utterances</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div><div><div id="utterances-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/blogs/img/MyProfilePicture.JPG" onerror="this.onerror=null;this.src='/blogs/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Stanley Zheng</div><div class="author-info-description">Hi, I am Stanley. I am currently a CS student in the University of Wisconsin-Madison. </div><div class="site-data"><a href="/blogs/archives/"><div class="headline">Articles</div><div class="length-num">55</div></a><a href="/blogs/tags/"><div class="headline">Tags</div><div class="length-num">15</div></a><a href="/blogs/categories/"><div class="headline">Categories</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/S-tanley"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://s-tanley.github.io" target="_blank" title="Homepage"><i class="fas fa-id-badge" style="color: #808080;"></i></a><a class="social-icon" href="https://github.com/S-tanley" target="_blank" title="Github"><i class="fab fa-github" style="color: #808080;"></i></a><a class="social-icon" href="mailto:zbowen936@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #808080;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">Summary</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract-Introduction-Background"><span class="toc-number">1.1.</span> <span class="toc-text">Abstract &amp; Introduction &amp; Background</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Challenges-and-Proposed-Solutions"><span class="toc-number">1.2.</span> <span class="toc-text">Challenges and Proposed Solutions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ORCA-Design-Implementation"><span class="toc-number">1.3.</span> <span class="toc-text">ORCA Design &amp; Implementation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Evaluation-Related-Work-and-Discussion-Conclusion"><span class="toc-number">1.4.</span> <span class="toc-text">Evaluation &amp; Related Work and Discussion &amp; Conclusion</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">Thoughts</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Confusion"><span class="toc-number">2.1.</span> <span class="toc-text">Confusion</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#About-Figure-1"><span class="toc-number">2.1.1.</span> <span class="toc-text">About Figure 1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#About-batching"><span class="toc-number">2.1.2.</span> <span class="toc-text">About batching</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#How-to-batch-requests-together"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">How to batch requests together</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#About-Figure-5"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">About Figure 5</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#About-the-scheduling-algorithms"><span class="toc-number">2.1.3.</span> <span class="toc-text">About the scheduling algorithms</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#first-come-first-served-FCFS"><span class="toc-number">2.1.3.1.</span> <span class="toc-text">first-come-first-served (FCFS)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#max-tokens-attribute"><span class="toc-number">2.1.3.2.</span> <span class="toc-text">max_tokens attribute</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Ideas"><span class="toc-number">2.2.</span> <span class="toc-text">Ideas</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#About-the-iteration-level-scheduling"><span class="toc-number">2.2.1.</span> <span class="toc-text">About the &quot;iteration level scheduling &quot;</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/blogs/2025/10/15/Statistics---Why%20We%20Use%20the%20t-Distribution%20to%20Estimate%20the%20Population%20Mean/" title="Statistics---Why We Use the t-Distribution to Estimate the Population Mean">Statistics---Why We Use the t-Distribution to Estimate the Population Mean</a><time datetime="2025-10-15T05:00:00.000Z" title="Created 2025-10-15 00:00:00">2025-10-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/blogs/2025/10/14/Statistics---Unbiasedness%20and%20Consistency/" title="Statistics---Unbiasedness and Consistency">Statistics---Unbiasedness and Consistency</a><time datetime="2025-10-14T05:00:00.000Z" title="Created 2025-10-14 00:00:00">2025-10-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/blogs/2025/10/14/Statistics---Sufficiency/" title="Statistics---Sufficiency">Statistics---Sufficiency</a><time datetime="2025-10-14T05:00:00.000Z" title="Created 2025-10-14 00:00:00">2025-10-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/blogs/2025/10/14/Statistics---%E5%85%8B%E6%8B%89%E9%BB%98-%E6%8B%89%E5%A5%A5%E4%B8%8B%E9%99%90%20(Crame%CC%81r-Rao%20Lower%20Bound,%20CRLB)/" title="Statistics---克拉默-拉奥下限（Cramér-Rao Lower Bound, CRLB）">Statistics---克拉默-拉奥下限（Cramér-Rao Lower Bound, CRLB）</a><time datetime="2025-10-14T05:00:00.000Z" title="Created 2025-10-14 00:00:00">2025-10-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/blogs/2025/10/12/Operating%20Systems%20Notes:%20The%20Path%20to%20the%20Kernel/" title="Operating Systems Notes: The Path to the Kernel">Operating Systems Notes: The Path to the Kernel</a><time datetime="2025-10-12T05:00:00.000Z" title="Created 2025-10-12 00:00:00">2025-10-12</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By Stanley Zheng</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll to Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/blogs/js/utils.js"></script><script src="/blogs/js/main.js"></script><div class="js-pjax"><script>(async () => {
  const showKatex = () => {
    document.querySelectorAll('#article-container .katex').forEach(el => el.classList.add('katex-show'))
  }

  if (!window.katex_js_css) {
    window.katex_js_css = true
    await btf.getCSS('https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css')
    if (false) {
      await btf.getScript('https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js')
    }
  }

  showKatex()
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = {"data-mapping":"pathname","data-input-position":"top","data-reactions-enabled":1,"data-strict":1}

  const getGiscusTheme = theme => theme === 'dark' ? 'dark' : 'light'

  const createScriptElement = config => {
    const ele = document.createElement('script')
    Object.entries(config).forEach(([key, value]) => {
      ele.setAttribute(key, value)
    })
    return ele
  }

  const loadGiscus = (el = document, key) => {
    const mappingConfig = isShuoshuo
      ? { 'data-mapping': 'specific', 'data-term': key }
      : { 'data-mapping': (option && option['data-mapping']) || 'pathname' }

    const giscusConfig = {
      src: 'https://giscus.app/client.js',
      'data-repo': 'S-tanley/blogs',
      'data-repo-id': 'R_kgDOOPJb6Q',
      'data-category-id': 'DIC_kwDOOPJb6c4CohlA',
      'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
      'data-reactions-enabled': '1',
      crossorigin: 'anonymous',
      async: true,
      ...option,
      ...mappingConfig
    }

    const scriptElement = createScriptElement(giscusConfig)

    el.querySelector('#giscus-wrap').appendChild(scriptElement)

    if (isShuoshuo) {
      window.shuoshuoComment.destroyGiscus = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const changeGiscusTheme = theme => {
    const iframe = document.querySelector('#giscus-wrap iframe')
    if (iframe) {
      const message = {
        giscus: {
          setConfig: {
            theme: getGiscusTheme(theme)
          }
        }
      }
      iframe.contentWindow.postMessage(message, 'https://giscus.app')
    }
  }

  btf.addGlobalFn('themeChange', changeGiscusTheme, 'giscus')

  if (isShuoshuo) {
    'Giscus' === 'Giscus'
      ? window.shuoshuoComment = { loadComment: loadGiscus }
      : window.loadOtherComment = loadGiscus
    return
  }

  if ('Giscus' === 'Giscus' || !false) {
    if (false) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
    else loadGiscus()
  } else {
    window.loadOtherComment = loadGiscus
  }
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null
  const getUtterancesTheme = theme => theme === 'dark' ? 'photon-dark' : 'github-light'

  const loadUtterances = (el = document, key) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyUtterances = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const config = {
      src: 'https://utteranc.es/client.js',
      repo: 'S-tanley/blogs',
      theme: getUtterancesTheme(document.documentElement.getAttribute('data-theme')),
      crossorigin: 'anonymous',
      async: true,
      ...option,
      'issue-term': isShuoshuo ? key : (option && option['issue-term']) || 'pathname'
    }

    const ele = document.createElement('script')
    Object.entries(config).forEach(([key, value]) => ele.setAttribute(key, value))
    el.querySelector('#utterances-wrap').appendChild(ele)
  }

  const changeUtterancesTheme = theme => {
    const iframe = document.querySelector('#utterances-wrap iframe')
    if (iframe) {
      const message = {
        type: 'set-theme',
        theme: getUtterancesTheme(theme)
      };
      iframe.contentWindow.postMessage(message, 'https://utteranc.es')
    }
  }

  btf.addGlobalFn('themeChange', changeUtterancesTheme, 'utterances')

  if (isShuoshuo) {
    'Giscus' === 'Utterances'
      ? window.shuoshuoComment = { loadComment: loadUtterances }
      : window.loadOtherComment = loadUtterances
    return
  }
  
  if ('Giscus' === 'Utterances' || !false) {
    if (false) btf.loadComment(document.getElementById('utterances-wrap'), loadUtterances)
    else loadUtterances()
  } else {
    window.loadOtherComment = loadUtterances
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/blogs/js/search/local-search.js"></script></div></div></body></html>